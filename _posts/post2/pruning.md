Why Pruning?

As deep learning’s popularity continues to explode, an increasing number of companies
have begun to rely on cutting-edge deep learning algorithms to deliver core product
functionality to users. However, developing and deploying large neural networks is an
extremely resource-intensive process that quickly becomes a core business cost for
these companies. Furthermore, many applications of deep learning, ranging from
mobile phone applications to robotic control systems, require rapid model inference.
This is very difficult to accomplish with large and unwieldy models.
Ongoing research in neural network pruning aims to address many of these issues by
exploring techniques for removing extraneous parameters from neural networks without
compromising their accuracy. Smaller networks naturally require less space and are
faster and cheaper to both train and deploy, making this line of work particularly
important for those in industry actively seeking to reduce the costs surrounding model
development and deployment.

Deep Learning 101
Before we dive into things, let’s quickly cover some important terms and concepts that
will be referred to in subsequent sections. Deep learning models, also known as neural
networks, are a class of machine learning algorithms that have found immense success
in recent years. For simplicity’s sake, we will focus on
