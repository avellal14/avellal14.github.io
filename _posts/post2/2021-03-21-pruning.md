---
layout: post
title:  "A Primer on Neural Network Pruning"
date:   2021-03-21 10:00:00 +0700
categories: jekyll update
---

**Why Pruning?**

As deep learning’s popularity continues to explode, an increasing number of companies
have begun to rely on cutting-edge deep learning algorithms to deliver core product
functionality to users. However, developing and deploying large neural networks is an
extremely [resource-intensive process](https://future.a16z.com/new-business-ai-different-traditional-software/)
that quickly becomes a core business cost for these companies. Furthermore, many applications of deep learning, 
ranging from mobile phone applications to robotic control systems, require rapid model inference.
This is very difficult to accomplis with large and unwieldy models.

Ongoing research in neural network pruning aims to address many of these issues by
exploring techniques for removing extraneous parameters from neural networks without
compromising their accuracy. Smaller networks naturally require less space and are
faster and cheaper to both train and deploy, making this line of work particularly
important for those in industry actively seeking to reduce the costs surrounding model
development and deployment.


**Deep Learning 101**

Before we dive into things, let’s quickly cover some important terms and concepts that
will be referred to in subsequent sections. *Deep learning models*, also known as *neural
networks*, are a class of machine learning algorithms that have found immense success
in recent years. For simplicity’s sake, we will focus on *supervised learning tasks*, where
we are provided with a set of datapoints and a corresponding set of labels describing
these datapoints. Supervised learning with neural networks has been most successful
in the domains of vision (image data) and language (text data). The goal is to learn a
function, *f*, that maps a given datapoint (such as an image or piece of text) to its
corresponding label. *f* is parameterized by a neural network, which is defined by
composing many matrix multiplications and subsequent nonlinearities on top of each
other. For example, we can define *f(x)* for an input *x* and label *y* as follows, where *w_1* and
*w_2* are coefficient matrices known as weights and *h_1* and *h_2* are nonlinear functions that
are applied elementwise to their inputs: *f(x) = h_2(w_2(h_1(w_1(x)))*. This can also be viewed as
passing an input through consecutive layers of interconnected neurons where the
connections between neurons are quantified using the weight matrices (*w_1* and *w_2*).
Training a neural network refers to the process of iteratively updating the network’s
weights so that its predictions on the given datapoints closely match the provided data
labels. Neural networks can be trained using a variety of different optimization
algorithms that provide instructions on how to update the weights in order to bestmatch the network 
outputs to the labels. Model inference refers to the process of obtaining a trained network’s predictions on 
new data in production.


**The Lottery Ticket Hypothesis**

Most of the recent research in model pruning was kickstarted by Jonathan Frankle and
Michael Carbin’s 2018 paper titled [The Lottery Ticket Hypothesis: Finding Sparse,
Trainable, Neural Networks](https://arxiv.org/pdf/1803.03635.pdf). The titular Lottery Ticket Hypothesis proposes that we view
large neural networks as ensembles of many smaller subnetworks, a few of which will
perform extremely well on the task at hand. However, there is no way to identify these
winning subnetworks apriori, so we use really large networks to get more shots on goal
and increase the likelihood that one of the component subnetworks is a winner. If we
can find these winning subnetworks, or “lottery tickets”, then we can discard, or “prune”
the rest of the network without sacrificing the accuracy. In practice, this is done using an
iterative process with the following steps:

1. Initialize network weights and train to convergence on the given task
2. Identify smallest weights and prune them by setting them to zero
3. Repeat steps 1 and 2 with the remaining weights until the desired proportion of weights have been pruned

While this approach does succeed in identifying winning subnetworks that amazingly
manage to achieve similar performance to the original network, it does not offer any
true cost savings for a few reasons. First of all, the iterative pruning process is actually
more time-intensive than the standard training process for a neural network because it
requires repeated re-initialization of the network weights. This is equivalent to re-training
the network from scratch many times until the desired subnetwork size is reached. The
hope of course is that the more expensive training leads to large savings during
inference time. However, this turns out not to be the case. Frankle’s pruning method
does not actively remove layers or weights from a network. Instead, it prunes weights by
simply setting them to zero. While this does produce some space savings due to the
storage of zeros instead of large floating point values, it does not lead to any speed
improvement during inference. From a hardware standpoint, zero weights and non-zero
weights look exactly the same. All the matrix multiplications involving the zero weights
still have to be executed by the hardware because it has no way of determining which
weights are zero and which are not ahead of time. Thus, networks pruned using this
method do not offer any tangible reduction in the time and GPU compute required to
perform inference.

A few follow up works from Frankle et al. ([1](https://arxiv.org/pdf/1903.01611.pdf), [2](https://arxiv.org/pdf/2002.10365.pdf)) along with another paper by [Zhou et al.](https://papers.nips.cc/paper/2019/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf) determine that while it is not necessary to re-initialize the weights completely during pruning, it is necessary to reset them their state in one of the earliest iterations of training. Setting all the pruned weights to zero appears to be crucial as well. While these papers do not offer any concrete speed improvements for either training or inference, they inform a variety of future research directions that are extremely promising. For example, research by [Morcos et al.](https://research.fb.com/wp-content/uploads/2019/12/One-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers.pdf) has found that lottery tickets learned for one dataset
using a specific optimization algorithm can also transfer to other datasets and optimization algorithms. This suggests that these subnetworks could be encoding specific structure and connectivity characteristics that could be invariant to the particulars of a dataset or optimization procedure. Meanwhile, [Yu et al.](https://arxiv.org/pdf/1906.02768.pdf) find that Frankle’s work extends beyond vision tasks and is effective on language and reinforcement learning tasks as well. The real hope here is that continued study of lottery tickets and their properties will eventually give us a way to identify a few golden ticket subnetworks whose structure and initial weight values work extremely well across a wide array of datasets. We can then directly initialize these subnetworks and realize significant savings during both training and inference time. Though we are not yet at this point, papers that have been published over the past few months seem to be moving in the right direction.

<!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
